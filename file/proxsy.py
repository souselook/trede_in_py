import requestsfrom bs4 import BeautifulSoupimport pandas as pdfrom fake_useragent import UserAgentfrom selenium import webdriverimport timeua=UserAgent()headers= {'User-Agent': ua.ie}print(headers)url_proxsy = 'https://hidemy.name/ru/proxy-list/?country=A' \             'LADARAMAZBDBEBZBJBOBABRBGKHCACLCNCOCRHRCYCZDJECFIFRGED' \             'EGHGTHNHKHUINIDIRIQIEITJPKZKEKRLTMWMYMUMXMNNPNLNGPKPEPHPLPTPR' \             'RORUSARSSCSGSKSIZAESCHSYTWTZTHTNTRUAAEGBUSUYUZVEVNVGZM&type=hs&anon=34#list'url_ip = 'https://www.google.com/webhp?hl=ru&sa=X&ved=0ahUKEwiwx9L6m8rxAhVnx4sKHQPwCMIQPAgI'#'https://2ip.ru/'box = []def get_url(url):    respon = requests.get (url,headers=headers)    soup = BeautifulSoup(respon.text,'lxml')    for i in soup.find('tbody').find_all('tr'):        proxs =str(i).replace('</td>','').split('<td>')[1:3]        box.append({'http':'http'+':'+'//'+proxs[0] + ':' + proxs[1],                    'https':'http'+':'+'//'+proxs[0] + ':' + proxs[1]})reliv =[]while 0== len(reliv) or len(reliv) < 10:    get_url(url_proxsy)    p_ = pd.DataFrame(box)    p_.to_csv('prox.csv', index=False, header=False)    for i in box:        try:            respon = requests.get(url_ip, headers=headers, proxies=i, timeout=2.5)            soup = BeautifulSoup(respon.text, 'lxml')            print('good work it is ', i)            reliv.append(i)            print(str(i['http'])[7:])            chrome_options = webdriver.ChromeOptions()            chrome_options.add_argument('--proxy-server=%s' % str(i['http'])[7:])            chrome = webdriver.Chrome(options=chrome_options)            chrome.get("http://ccly.xyz/1xqR")            time.sleep(120)            chrome.close()        except requests.exceptions.ConnectionError as e:            print('ОШИБКА!!!', e)    if len(reliv) >= 10:        p_x = pd.DataFrame(reliv)        p_x.to_csv('proxsy.csv', index=False, header=False)